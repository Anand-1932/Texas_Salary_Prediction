{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anand-1932/Texas_Salary_Prediction/blob/main/PRCP_1024_TexasSalaryPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ve9VS8EJIM0"
      },
      "source": [
        "<h1>CONTENTS<span class=\"tocSkip\"></span></h1>\n",
        "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Business Case\" data-toc-modified-id=\"Business Case-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Business Case</a></span></li>\n",
        "<li><span><a href=\"#Domain Analysis\" data-toc-modified-id=\"Domain Analysis-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Domain Analysis</a></span></li>\n",
        "<li><span><a href=\"#Importing Libraries\" data-toc-modified-id=\"Importing Libraries-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Importing Libraries</a></span></li>\n",
        "<li><span><a href=\"#Loading Dataset\" data-toc-modified-id=\"Loading Dataset-3\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Loading Dataset</a></span></li>\n",
        "<li><span><a href=\"#Basic Checks\" data-toc-modified-id=\"Basic Checks\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Basic Checks</a></span></li>\n",
        "<li><span><a href=\"#Exploratory Data Ananlysis\" data-toc-modified-id=\"#Exploratory Data Ananlysis\"\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Exploratory Data Ananlysis</a></span></li>\n",
        "<li><span><a href=\"#Data Preprocessing\" data-toc-modified-id=\"#Data Preprocessing\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Data Preprocessing</a></span></li>\n",
        "<li><span><a href=\"#Feature Engineering\" data-toc-modified-id=\"Feature Engineering\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Feature Engineering</a></span></li>\n",
        "<li><span><a href=\"#Split Data Into Independent & Dependent Variable\" data-toc-modified-id=\"Split Data Into Independent & Dependent Variable\"><span class=\"toc-item-num\">9&nbsp;&nbsp;</span>Split Data Into Independent & Dependent Variable</a></span></li>\n",
        "<li><span><a href=\"#Model Building\" data-toc-modified-id=\"Model Building><span class=\"toc-item-num\">10&nbsp;&nbsp;</span>Model Building</a></span></li>\n",
        "<li><span><a href=\"#Hyperparameter Tunning\" data-toc-modified-id=\"Hyperparameter Tunning><span class=\"toc-item-num\">11&nbsp;&nbsp;</span>Hyperparameter Tunning</a></span></li>\n",
        "<li><span><a href=\"#Model Selection Baeed on F1 Score\" data-toc-modified-id=\"Model Selection Baeed on R2 Score><span class=\"toc-item-num\">12&nbsp;&nbsp;</span>Model Selection Baeed on R2 Score</a></span></li>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEx3HZOWJl0q"
      },
      "source": [
        "# **BUSINESS CASE**\n",
        "\n",
        "In today's competitive job market, accurate salary predictions are essential for business, job seekers, and HR professionals. A Salary Prediction Model leverages data-driven insights to estimates salaries based on factors such as experience, education, industry, location, and job role. This model enhances HR decision-making, salary benchmarking, talent acquisition, and employee retention strategies.\n",
        "\n",
        "We have Texas Salary Prediction Model leverages machine learning and data analytics to estimates salaries across different job roles, industries, and experiences within the texas. By analyzing historical salary trends, economic factors, and job market dynamics specific to texas. this model help oragnizations optimize hiring budgets and enhance worforce planning.\n",
        "\n",
        "With this regression model, a job seeker can predict the salary across the different domain by inputing their education, experience, job they want, and the location of the industries. It can be useful in the salary negotiation and also help them to get fair salary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ96Fo4uJrNg"
      },
      "source": [
        "# **DOMAIN ANALYSIS**\n",
        "\n",
        "While doing this project we are going to use dataset which contain the salary of different job role of the people of texas, this dataset is pretty huge it contain more than 149400 column and 21 rows.\n",
        "  *   From the given dataset we will create a machine learning model which\n",
        "      will able to predict what will be the salary of the person based on the given input to the model.\n",
        "\n",
        "  *   This dataset contain 21 columns those are mentioned below, we will go through each one of the feature to gain insights and what is the the weights of the columns one the target column.\n",
        "\n",
        "      1. AGENCY\n",
        "\n",
        "          *  Unique identifier code for a government agency.\n",
        "          *  Some agencies may have higher pay scales due to budget allocations\n",
        "             or specialized roles.\n",
        "\n",
        "      2. AGENCY NAME\n",
        "\n",
        "          *  Full name of the agency employing the individual.\n",
        "          *  Different agencies have varying salary structures based on their\n",
        "             funding and responsibilities.\n",
        "\n",
        "      3. LAST NAME\n",
        "\n",
        "          *  Employee’s last name.\n",
        "          *  No direct impact on the salary column, it is used only for   \n",
        "             record-keeping.\n",
        "\n",
        "      4. FIRST NAME\n",
        "          \n",
        "          *  Employee’s first name.\n",
        "          *  No direct impact on the salary column, it is used only for   \n",
        "             record-keeping.\n",
        "\n",
        "      5. MI (Middle Initial)\n",
        "\n",
        "          *  Middle initial of the employee’s name.\n",
        "          *  No direct impact on the salary column, it is used only for\n",
        "             identification purposes.\n",
        "        \n",
        "      6. CLASS CODE\n",
        "\n",
        "          *  Code representing an employee’s job classification.\n",
        "          *  Directly related to salary as it determines job role and seniority\n",
        "             level.\n",
        "\n",
        "      7. CLASS TITLE\n",
        "\n",
        "          *  Job title or position of the employee.\n",
        "          *  Strong correlation with salary as higher job titles generally mean\n",
        "             higher salaries.\n",
        "\n",
        "      8. ETHNICITY\n",
        "\n",
        "          *  Ethnic background of the employee.\n",
        "          *  Can be used for diversity and pay equity analysis.\n",
        "\n",
        "      9. GENDER\n",
        "\n",
        "          *  Gender of the employee (Male/Female/Other).\n",
        "          *  Helps in analyzing gender pay disparities.\n",
        "      \n",
        "      10. STATUS\n",
        "\n",
        "          *  Employment status (Full-Time, Part-Time, Contract, etc.).\n",
        "          *  Full-time employees generally earn more than part-time or contract\n",
        "             workers.\n",
        "\n",
        "      11. EMPLOY DATE\n",
        "\n",
        "          *  Date when the employee started working.\n",
        "          *  More experienced employees (earlier hire dates) may have higher\n",
        "             salaries.\n",
        "\n",
        "      12. HRLY RATE\n",
        "\n",
        "          *  Hourly wage of the employee.\n",
        "          *  Directly contributes to total annual compensation.\n",
        "\n",
        "      13. HRS PER WK\n",
        "\n",
        "          *  Number of hours worked per week.\n",
        "          *  Employees working more hours earn higher total compensation.\n",
        "\n",
        "      14. MONTHLY\n",
        "      \n",
        "          *  Employee’s monthly salary.\n",
        "          *  Provides insight into how annual salaries are structured.\n",
        "\n",
        "      15. ANNUAL\n",
        "\n",
        "          *  Employee’s total annual salary.\n",
        "          *  The main variable we are predicting in our salary model.\n",
        "\n",
        "      16. STATE NUMBER\n",
        "\n",
        "          *  State-level identifier for employees.\n",
        "          *  No direct impact; useful for tracking employee records.\n",
        "\n",
        "      17. duplicated\n",
        "\n",
        "          *  Indicates whether the record is a duplicate entry.\n",
        "          *  Helps in data cleaning by removing duplicate records.\n",
        "\n",
        "      18.  multiple_full_time_jobs\n",
        "\n",
        "          *  Indicates whether an employee holds multiple full-time jobs.\n",
        "          *  Could lead to unusually high total compensation.\n",
        "\n",
        "      19. combined_multiple_jobs\n",
        "\n",
        "          *  Indicates whether salaries from multiple jobs have been combined.\n",
        "          *  Important for ensuring accurate salary calculations.\n",
        "\n",
        "      20. summed_annual_salary\n",
        "\n",
        "          *  Total annual salary when combining multiple job positions.\n",
        "          *  Helps understand total earnings of employees with multiple jobs.\n",
        "\n",
        "      21.  hide_from_search\n",
        "\n",
        "          *  Indicates whether an employee’s salary information is hidden from public searches.\n",
        "          *  No direct impact on salary prediction but affects data visibility.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T63QJ7hKJwKS"
      },
      "source": [
        "# **IMPORTING LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYuAtP2ZqWTC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poR94qaLlhKB"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_absolute_error, r2_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykAgCyOaJ2ie"
      },
      "source": [
        "# **LOADING DATASET**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJiDMj6bi4oI"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/salary.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NlT0k-4g3WuB"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4QQSufQJ7Il"
      },
      "source": [
        "# **BASIC CHECKS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxQhjx5UjPO9"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Em48jnYtjTM1"
      },
      "outputs": [],
      "source": [
        "df.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-YRGrd_GV25"
      },
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmPFryHkIggu"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jB5D1V46ImLm"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7QHhHk1IoTc"
      },
      "outputs": [],
      "source": [
        "df.describe(include='object')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUthI6g2I9nq"
      },
      "source": [
        "# Insights from Basic checks\n",
        "\n",
        "\n",
        "*   This dataset contain 149481 rows and 21 columns.\n",
        "\n",
        "*   duplicated, Multiple_full_times_jobs, combined_multiple_jobs, summend_annual_salary and hide_from_search column contain null values\n",
        "*   Dataset contain 6 float(64) datatype column, 2 int(64) datatype column and 13 object datatype column.\n",
        "\n",
        "\n",
        "*   Target column in Annual have mean of 50714.210973, min value of 600 and maximum salary of 553500.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvsBTKB8I9Wy"
      },
      "source": [
        "# **EXPLORATORY DATA ANALYSIS (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbYyvuAikg0M"
      },
      "source": [
        "**UNIVARIATE ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rmc5hulaMMf2"
      },
      "outputs": [],
      "source": [
        "df['AGENCY'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFNX6m-jI2Rj"
      },
      "outputs": [],
      "source": [
        "sns.histplot(df['AGENCY'],kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xQ_mbqIdHlo-"
      },
      "outputs": [],
      "source": [
        "# AGENCY NAME\n",
        "plt.figure(figsize=(20,5))\n",
        "df['AGENCY NAME'].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2GTwKXr9Thbv"
      },
      "source": [
        "Most of the 75% datapoints are from the following departments\n",
        "\n",
        "*   HEALTH AND HUMAN SERVICES COMMISSION\n",
        "\n",
        "*   TEXAS DEPARTMENT OF CRIMINAL JUSTICE\n",
        "\n",
        "*   TEXAS DEPARTMENT OF TRANSPORTATION\n",
        "*   DEPARTMENT OF FAMILY AND PROTECTIVE SERVICES\n",
        "\n",
        "\n",
        "*   DEPARTMENT OF PUBLIC SAFETY\n",
        "\n",
        "\n",
        "*   TEXAS WORKFORCE COMMISSION\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F2p5CFNxUbbS"
      },
      "outputs": [],
      "source": [
        "top_20_categories = df['CLASS TITLE'].value_counts().nlargest(20)\n",
        "\n",
        "# Plotting bar chart for the top 20 categories\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=top_20_categories.values, y=top_20_categories.index, palette=\"viridis\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.xlabel(df['CLASS TITLE'])\n",
        "plt.title(f\"Top 20 Most Frequent Categories in CLASS TITLE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YdxWTGZfOGd"
      },
      "outputs": [],
      "source": [
        "# ETHNICITY\n",
        "plt.figure(figsize=(20,5))\n",
        "df['ETHNICITY'].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0zZmw0ch8A7"
      },
      "outputs": [],
      "source": [
        "# GENDER\n",
        "plt.figure(figsize=(10,5))\n",
        "df['GENDER'].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOPh2w73il89"
      },
      "outputs": [],
      "source": [
        "# STATUS\n",
        "plt.figure(figsize=(10,5))\n",
        "df['STATUS'].value_counts().plot(kind='bar')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qm2W8Rzui8ee"
      },
      "outputs": [],
      "source": [
        "# HRLY RATE\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['HRLY RATE'],kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAe4Ef73jwMM"
      },
      "outputs": [],
      "source": [
        "# HRS PER WK\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['HRS PER WK'],kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1FQVrLLvjv99"
      },
      "outputs": [],
      "source": [
        "# MONTHLY\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['MONTHLY'],kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFZ_5VYYkGml"
      },
      "outputs": [],
      "source": [
        "# ANNUAL\n",
        "plt.figure(figsize=(10,5))\n",
        "sns.histplot(df['ANNUAL'],kde=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVnT_qIDlc8i"
      },
      "outputs": [],
      "source": [
        "# EMPLOY DATE\n",
        "sns.lineplot(x=df['EMPLOY DATE'],y=df['ANNUAL'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVSL_Rlskoq-"
      },
      "source": [
        "**BIVARIATE ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIxUgy_wkPlk"
      },
      "outputs": [],
      "source": [
        "# HRLY RATE AND ANNUAL\n",
        "sns.scatterplot(x=df['ANNUAL'],y=df['HRLY RATE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azKK6MhrkPDs"
      },
      "outputs": [],
      "source": [
        "# GENDER AND HRLY RATE\n",
        "sns.barplot(x=df['GENDER'],y=df['HRLY RATE'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLM2t7lTsB2j"
      },
      "outputs": [],
      "source": [
        "# GENDER VS ANNUAL\n",
        "sns.barplot(x=df['GENDER'],y=df['ANNUAL'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUz9fsX4sM1n"
      },
      "outputs": [],
      "source": [
        "# HRS PER WK AND ANNUAL\n",
        "sns.scatterplot(y=df['HRS PER WK'], x=df['ANNUAL'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faTJIxC_tl6V"
      },
      "outputs": [],
      "source": [
        "# STATUS VS SALARY\n",
        "sns.barplot(x=df['STATUS'],y=df['ANNUAL'])\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMjuS5kR6zcw"
      },
      "source": [
        "**MULTIVARIATE ANALYSIS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcyYrEyNuLG7"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(df.select_dtypes(include='number').corr(),annot=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9WV0A-T04uX"
      },
      "source": [
        "**Insights from EDA**\n",
        "\n",
        "*   None of the feature shows normal distribution.\n",
        "\n",
        "*   Almost in every feature few of sub-category contains more than 85% of the data points.\n",
        "\n",
        "*   Dataset contain more male candidate than female candidate.\n",
        "\n",
        "*   In STATUS column, classified regular point full-time contains more than 98% of the data points.\n",
        "\n",
        "*   Male candidate get more HRLY RATE than female candidate.\n",
        "*   male candidate get more salary than female candidate.\n",
        "\n",
        "\n",
        "*   People with highest salary mostly work 40 hours per week\n",
        "\n",
        "\n",
        "*   ERF- Exempt Regular Full Time job status has highest salary.\n",
        "\n",
        "\n",
        "*   HRS PER WK and HRLY RATE shows good coorelation with ANNUAL(Target) Feature.\n",
        "\n",
        "\n",
        "*   MONTHLY columns has 100% coorelation with ANNUAL(Target) Feature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b5GQLgh2ydF"
      },
      "source": [
        "# **DATA PREPROCESSING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9HHCbkg23FG"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GiJQWGoD23B7"
      },
      "outputs": [],
      "source": [
        "# checking for null value\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BLswxpu65sV"
      },
      "source": [
        "**List of column contain null value**\n",
        "\n",
        "*   duplicated\n",
        "\n",
        "*   multiple_full_time_jobs\n",
        "*   combined_multiple_jobs\n",
        "\n",
        "\n",
        "*   summed_annual_salary\n",
        "\n",
        "*   hide_from_search\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhfGGNZk2294"
      },
      "outputs": [],
      "source": [
        "# checking for duplicated value\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-ovXDkf7Ucb"
      },
      "source": [
        "**Dataset contain no duplicated value**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZQzjAVL226I"
      },
      "outputs": [],
      "source": [
        "# plotting boxplot to find outliers.\n",
        "plt.figure(figsize=(10,10))\n",
        "plotnumber=1\n",
        "\n",
        "for i in df.select_dtypes(include='number').columns:\n",
        "  if plotnumber<=8:\n",
        "\n",
        "    ax=plt.subplot(4,2,plotnumber)\n",
        "    sns.boxplot(x=df[i])\n",
        "    plt.xlabel(i,fontsize=10)\n",
        "    plt.ylabel('count',fontsize=10)\n",
        "\n",
        "    plotnumber+=1\n",
        "\n",
        "plt.tight_layout()  #to fit the graph properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CEdoNVs12215"
      },
      "outputs": [],
      "source": [
        "# calculating the percentage of missing value\n",
        "missing_value_percentage = (df.isnull().sum() / len(df)) *100\n",
        "missing_value_percentage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qo6v_cPs22yW"
      },
      "outputs": [],
      "source": [
        "# feature name  percentage of missing value\n",
        "# duplicated\t99.904336\n",
        "# multiple_full_time_jobs\t99.990634\n",
        "# combined_multiple_jobs\t99.935109\n",
        "# summed_annual_salary\t99.989296\n",
        "# hide_from_search  99.989296\n",
        "# in these above mentioned column more than 99% of the value is missing so their no need to impute the missing value what\n",
        "# we can simply do is we will remove these column in order to handle missing value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XKPIAiof22uu"
      },
      "outputs": [],
      "source": [
        "col_to_drop = ['duplicated','multiple_full_time_jobs','combined_multiple_jobs','summed_annual_salary','hide_from_search']\n",
        "df.drop(col_to_drop,axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttVan_f422q_"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZtMg_Od22nW"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJIbVkOW22eV"
      },
      "outputs": [],
      "source": [
        "# calculating percentage of outlier in dataset\n",
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "outlier_percentage = {}\n",
        "\n",
        "for col in numeric_cols:\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Count outliers\n",
        "    outlier_count = df[(df[col] < lower_bound) | (df[col] > upper_bound)].shape[0]\n",
        "    total_count = df.shape[0]\n",
        "    outlier_percentage[col] = (outlier_count / total_count) * 100\n",
        "\n",
        "# Print the percentage of outliers for each numerical column\n",
        "for col, perc in outlier_percentage.items():\n",
        "    print(f\"{col}: {perc:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ_j0t6cS6Uy"
      },
      "outputs": [],
      "source": [
        "# ANNUAL Column\n",
        "Q1 = df[\"ANNUAL\"].quantile(0.25)\n",
        "Q3 = df[\"ANNUAL\"].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "\n",
        "# Define outlier threshold\n",
        "lower_bound = Q1 - 1.5 * IQR\n",
        "upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "# No. of outlier in target column\n",
        "outlier = df[(df['ANNUAL'] < lower_bound) | (df['ANNUAL'] > upper_bound)]\n",
        "outlier_counts = outlier.shape[0]\n",
        "outlier_count, outlier[[\"AGENCY NAME\", \"CLASS TITLE\", \"ANNUAL\"]].head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3Y7jDJMSApg"
      },
      "source": [
        "**Outlier Analysis in ANNUAL(Target) column**\n",
        "\n",
        "\n",
        "*   Total outliers identified : 9031(6.04%)\n",
        "\n",
        "*   High paying job roles\n",
        "\n",
        "      *  Judges (Retired): $114,549\n",
        "\n",
        "      *  Directors (IV level): $120,000–$154,788\n",
        "\n",
        "      *  Legislative Officials/Administrators: $96,000–$133,800\n",
        "\n",
        "Above mentioned job role are typically considered one of the most reputed job roles and also holds more salaries than expected, so they may not be the true outliers since these position hold high rank position in job role list, so we will not considered these as outliers but rather accept it as high-earning roles.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oU0jGbKK22Zp"
      },
      "outputs": [],
      "source": [
        "# Treating outlier in HRLY RATE and HRS PER WK\n",
        "# replacing value lower than lower_wisker with lower_wisker and\n",
        "# value higher than upper_wisker with upper_wisker.\n",
        "def wisker(col):\n",
        "  Q1,Q3=np.percentile(col,(25,75))\n",
        "  IQR=Q3-Q1\n",
        "  lw=Q1-(1.5*IQR)\n",
        "  uw=Q3+(1.5*IQR)\n",
        "  return lw,uw\n",
        "\n",
        "for i in ['HRLY RATE','HRS PER WK']:\n",
        "  lw,uw=wisker(df[i])\n",
        "  df[i]=np.where(df[i]<lw,lw,df[i])\n",
        "  df[i]=np.where(df[i]>uw,uw,df[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARN86sJf22U6"
      },
      "outputs": [],
      "source": [
        "# Plotting Box plot for these column once again to check wheather outliers are treated or not\n",
        "# plotting boxplot to find outliers.\n",
        "plt.figure(figsize=(10,5))\n",
        "plotnumber=1\n",
        "\n",
        "for i in ['HRLY RATE','HRS PER WK']:\n",
        "  if plotnumber<=2:\n",
        "\n",
        "    ax=plt.subplot(1,2,plotnumber)\n",
        "    sns.boxplot(x=df[i])\n",
        "    plt.xlabel(i,fontsize=10)\n",
        "    plt.ylabel('count',fontsize=10)\n",
        "\n",
        "    plotnumber+=1\n",
        "\n",
        "plt.tight_layout()  #to fit the graph properly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ct5M60bm22QF"
      },
      "outputs": [],
      "source": [
        "# Grouping by job title and calculating salary statistics\n",
        "salary_by_title = df.groupby(\"CLASS TITLE\")[\"ANNUAL\"].describe()\n",
        "\n",
        "# Finding roles with highest salary disparity\n",
        "salary_by_title[\"wage_gap\"] = salary_by_title[\"max\"] - salary_by_title[\"min\"]\n",
        "top_disparities = salary_by_title.sort_values(\"wage_gap\", ascending=False).head(10)\n",
        "\n",
        "top_disparities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRqRc1bnaEQ3"
      },
      "source": [
        "** Wage Disparities Between Managers & Employees**\n",
        "\n",
        "   *  The biggest salary gaps exist in:\n",
        "\n",
        "      1. Elected Officials Staff → $600 - $279,996 (gap: $279K)\n",
        "      2. Legislative Administrators → $6,000 - $276,500 (gap: $270K)\n",
        "      3. Executive Directors → $90,199 - $355,141 (gap: $264K)\n",
        "      4. Psychiatrists & Physicians → Major gaps ($180K+)\n",
        "      5. Chief Investment Officers → $225,000 - $450,000 (gap: $225K)\n",
        "\n",
        "\n",
        "* These disparities suggest high variance in seniority and responsibility within roles i.e., person with more respondibility and more expereience have more salaries than the person who are newly to that field.\n",
        "\n",
        "* Some low salaries person (e.g., $600 for Elected Staff) are might be the person who are intern or doing part-time job in that field or there might be possible chances that there may be some data errors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpbpe6XqfNiq"
      },
      "outputs": [],
      "source": [
        "# salaries trend over time\n",
        "df['EMPLOY DATE'] = pd.to_datetime(df['EMPLOY DATE'])\n",
        "df['EMPLOY DATE'] = df['EMPLOY DATE'].dt.year\n",
        "\n",
        "salary_change = df.groupby('EMPLOY DATE')['ANNUAL'].mean()\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.lineplot(x=salary_change.index , y=salary_change.values, marker='o', linestyle='-')\n",
        "plt.title('Salary Trend Over Time')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Salary')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3l-sZn-dPf1"
      },
      "source": [
        "**Trend in salaries**\n",
        "\n",
        "\n",
        "*   There is a steady increase in average salaries over time.\n",
        "*   Some dips might correspond to economic downturns or policy changes.\n",
        "*   The latest years show higher salaries, possibly due to inflation adjustments or competitive hiring.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YowGAKudrfm"
      },
      "source": [
        "# **FEATURE ENGINEERING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4_hFu7Hcz6X"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rIgf_7aGd3wf"
      },
      "outputs": [],
      "source": [
        "# Dropping column which are not contributing to the\n",
        "# prediction of the target such as LAST NAME, FIRST NAME AND MI\n",
        "df.drop(['LAST NAME','FIRST NAME','MI'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnIxeky7fN8B"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IKxfC8QfP5b"
      },
      "outputs": [],
      "source": [
        "# Dropping MONTHLY column as it as high coorelation with other independent column as well as\n",
        "# it almost duplicate of the target column\n",
        "# it signifies same information as ANNUAL column\n",
        "# only difference here is it shows monthly salary and target column shows annual salaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ajKL_Gxf7jA"
      },
      "outputs": [],
      "source": [
        "df.drop(['MONTHLY'], axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NgESaDtZgFFm"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Bt29nlmgGks"
      },
      "outputs": [],
      "source": [
        "# Dropping AGENCY and CLASS CODE column beacause it is nothing but code given to AGENCY NAME and CLASS TITLE\n",
        "df.drop(['AGENCY','CLASS CODE'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZbplj5IhAP8"
      },
      "outputs": [],
      "source": [
        "df.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtsLQHqnhBdA"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXdz_nq2hC1s"
      },
      "outputs": [],
      "source": [
        "# Droppind state number since it is no contributing to predict the target\n",
        "df.drop(['STATE NUMBER'],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ls9Qrn4shWYd"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FL_YZJ-HjDQN"
      },
      "source": [
        "# **SPLITTING DATA INTO INDEPENDENT AND DEPENDENT VARIABLE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYJwZDUdhX02"
      },
      "outputs": [],
      "source": [
        "features = [\"AGENCY NAME\", \"CLASS TITLE\", \"ETHNICITY\", \"GENDER\", \"STATUS\", \"HRS PER WK\", \"HRLY RATE\"]\n",
        "target = \"ANNUAL\"\n",
        "\n",
        "# Dropping rows with missing target values\n",
        "df_model = df.dropna(subset=[target])\n",
        "\n",
        "# Encoding categorical variables\n",
        "label_encoders = {}\n",
        "for col in [\"AGENCY NAME\", \"CLASS TITLE\", \"ETHNICITY\", \"GENDER\", \"STATUS\"]:\n",
        "    le = LabelEncoder()\n",
        "    df_model[col] = le.fit_transform(df_model[col].astype(str))\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Splitting dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(df_model[features], df_model[target], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7VbWjj8jXfB"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KYXza-HrCHs"
      },
      "source": [
        "# **MODEL BUILDING**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKiBqfl2rscv"
      },
      "source": [
        "**RANDOM FOREST**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojAnpRgal5K8"
      },
      "outputs": [],
      "source": [
        "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SurJqxWlrxA9"
      },
      "source": [
        "**LINEAR REGRESSION**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7o7AKN1rQqR"
      },
      "outputs": [],
      "source": [
        "model1 = LinearRegression()\n",
        "model1.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model1.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyuCmEEvs2PF"
      },
      "source": [
        "**SVR**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMKpgbQqsFFO"
      },
      "outputs": [],
      "source": [
        "model3 = SVR()\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model3.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxlXJgjDs8e6"
      },
      "source": [
        "**DECISION TREE**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-B3XeCUls7BY"
      },
      "outputs": [],
      "source": [
        "model4 = DecisionTreeRegressor()\n",
        "model4.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model4.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hbCNKD9yNGA"
      },
      "source": [
        "**Gradient Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPwq_nOWuUcE"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "model5 = GradientBoostingRegressor(n_estimators=500)\n",
        "model5.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model5.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BirYRysryr3W"
      },
      "source": [
        "**xgboost**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qBhDbbnuUPi"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor\n",
        "model6 = XGBRegressor(n_estimators=45,max_depth=5,learning_rate=0.5)\n",
        "model6.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model6.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtRBXvLV6pO2"
      },
      "source": [
        "# **HYPERPARAMETER TUNNING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzPrERbjtgEi"
      },
      "outputs": [],
      "source": [
        "# FOR RANDOMFORESTREGRESSOR\n",
        "# hyperparameter tunning\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "n_estimators=[int(x) for x in np.linspace(start=200,stop=2000,num=10)]\n",
        "max_features=['auto','sqrt','log2']\n",
        "max_depth=[int(x) for x in np.linspace(10,110,num=11)]\n",
        "min_samples_split=[2,5,18]\n",
        "min_samples_leaf=[1,2,4]\n",
        "\n",
        "random_grid={\n",
        "    'n_estimators':n_estimators,\n",
        "    'max_features':max_features,\n",
        "    'max_depth':max_depth,\n",
        "    'min_samples_split':min_samples_split,\n",
        "    'min_samples_leaf':min_samples_leaf\n",
        "}\n",
        "\n",
        "rf_rgr1=RandomForestRegressor(random_state=3)\n",
        "\n",
        "rf_cv=RandomizedSearchCV(estimator=rf_rgr1,scoring='r2',param_distributions=random_grid,n_iter=100,cv=4,\n",
        "                         verbose=2,random_state=3,n_jobs=-1)\n",
        "\n",
        "rf_cv.fit(X_train,y_train)\n",
        "rf_best_params=rf_cv.best_params_\n",
        "print(f\"Best parameters:, {rf_best_params})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SwnJ2HXzKupw"
      },
      "outputs": [],
      "source": [
        "# training model with best paramters\n",
        "model_RF= RandomForestRegressor(n_estimators=600,min_samples_split=2,min_samples_leaf=1,max_features='log2',max_depth=90)\n",
        "model_RF.fit(X_train,y_train)\n",
        "# Predictions\n",
        "y_pred = model_RF.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2vhSuGvNtKaG"
      },
      "outputs": [],
      "source": [
        "# for DecisionTree\n",
        "# hyperparamter tunning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "params={\n",
        "    \"criterion\":(\"mse\",\"mae\",\"friedman_mse\",\"poission\"),\n",
        "    \"splitter\":(\"best\",\"random\"),\n",
        "    \"max_depth\":(list(range(1,20))),\n",
        "    \"min_samples_split\":[2,3,4],\n",
        "    \"min_samples_leaf\":list(range(1,20))\n",
        "}\n",
        "\n",
        "tree_rgr=DecisionTreeRegressor()\n",
        "tree_cv=GridSearchCV(tree_rgr,params,scoring=\"r2\",n_jobs=-1,cv=5)\n",
        "tree_cv.fit(X_train,y_train)\n",
        "\n",
        "best_params=tree_cv.best_params_\n",
        "print(f\"Best parameters:, {best_params})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFLXlzWxzpcH"
      },
      "outputs": [],
      "source": [
        "# training model with best paramters\n",
        "model_DT= RandomForestRegressor(criterion='friedman_mse',max_depth=19,min_samples_leaf=1,min_samples_split=3)\n",
        "model_DT.fit(X_train,y_train)\n",
        "# Predictions\n",
        "y_pred = model_DT.predict(X_test)\n",
        "\n",
        "# Model evaluation\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "mae, r2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MODEL COMPARISION**\n",
        "\n",
        "\n",
        "*   Algorithm used in this project are linear regression, Random Forest, Decision Tree, SVR, Gradient Boosting and xgboost.\n",
        "\n",
        "*   Some of the algorithm which have highest R2 Score are mentioned below.\n",
        "\n",
        "*   Algorithm used :  MAE   |    R2_Score\n",
        "\n",
        "      *   Random Forest : 3121.11 | 92.22%\n",
        "\n",
        "      *   Decision Tree : 3220 | 91.23%\n",
        "\n",
        "      *   Gradient Boosting : 5878.21 | 89%\n",
        "\n",
        "      *   xgboost : 6837.89 | 79.9%\n",
        "\n",
        "\n",
        "*   Linear Regression doesn't work well with this dataset.\n",
        "*   Algorithm with lowest R2 Score is SVR which is -3% and MAE is 15662\n",
        "\n",
        "\n",
        "*   In conclusion, Random Forest provide us the best mae value as well as R2_score which is 3121.11 and 92.22% respectively.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7QGzDRsclCm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CHALLANGE FACED**\n",
        "\n",
        "\n",
        "*   Dataset Contain irrelevant column such as last_name, first_name etc we dropped these column in order to achieve better R2_score.\n",
        "\n",
        "*   Dataset is huge is it is computionally expensive while using some of the algorithm.\n",
        "\n",
        "*   We have perform hyperparameter tunning only for those algorithm which achieve R2_score of above 90% due to huge dataset it is computionally expensive to perform hyperparameter tunning for every algorithm.\n",
        "\n",
        "*   Some of the column have to be removed because it contain null value more than 95%\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_0mK_RfslCio"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONCLUSION**\n",
        "\n",
        "The salary prediction project for the Texas state government provides valuable insights into payroll patterns, wage disparities, and compensation trends. Our project is doing well as we achieve the R2 score of 92.22% with the Random Forest we also perform hyperparamter tunning but it doesn't make more changes in the R2_score. The analysis also highlights potential outliers, discrepancies between managerial and employee wages, and salary trends over time. this project serves as a powerful tool for workforce planning, budget optimization, and ensuring fair and transparent compensation policies within the Texas state government."
      ],
      "metadata": {
        "id": "eQdncDm-lCch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FUTURE SCOPE**\n",
        "\n",
        "The future scope of this salary prediction project is vast, with opportunities to enhance model performance, expand data sources, and provide valuable insights for business and policy applications.Feature engineering techniques, such as adding tenure-based salary trends or inflation-adjusted compensation, can refine predictions further.our dataset contains job role related to goverment officals majorly but if we can add other job role also such sales, marketing, IT field etc it will be further used globally for evry job role which will help HR, employee, management as well as company to make policy about pay. Additionally, applying time series forecasting models can help predict future salary trends based on economic conditions and policy changes. From a business and policy perspective, the project can aid government agencies in identifying wage disparities, optimizing payroll budgets, and making data-driven decisions for equitable salary adjustments."
      ],
      "metadata": {
        "id": "ow0oFr__vewb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XSwKvsyikF-m"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMg3qaFKfLi/QdNXe/HynEV",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}